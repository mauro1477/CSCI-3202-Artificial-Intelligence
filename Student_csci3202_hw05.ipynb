{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Mauro Vargas Jr\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "372b2e64a091bc5c1e72fcc5f697fe12",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "# CSCI 3202, Spring 2021:  Assignment 5  \n",
    "\n",
    "Shortcuts:  [top](#top) -- [1](#p1) | [1a](#p1a) | [1b](#p1b) | [1c](#p1c) | [1d](#p1d) | [1e](#p1e) | [1f](#p1f) | [1g](#p1g) -- [2](#p2) | [2a](#p2a) | [2b](#p2b) | [2c](#p2c) | [2d](#p2d) | [2e](#p2e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c31566edfa1fdd0af96343452430d5f3",
     "grade": false,
     "grade_id": "overview",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment Overview\n",
    "\n",
    "This assignment is an exercise in implementing and analyzing Markov Decision Processes (MDPs). Problem 1 asks you to code a solution to a specific scenario, while Problem 2 is a conceptual question which asks you to describe an MDP problem of your own design.\n",
    "\n",
    "Here's a summary of the tasks required and the associated points:\n",
    "\n",
    "| Problem #  | Tasks                                                  | Points  |\n",
    "|:---        |:---                                                    |:---:    |\n",
    "| 1a         | Code: Complete implementation of `MDP` class           | 10      |\n",
    "| 1b         | Code: Implement `value_iteration` and `find_policy`    | 5       |\n",
    "| 1c         | Code and create: Generate and illustrate optimal path  | 5       |\n",
    "| 1d         | Written: analyze policy                                | 5       |\n",
    "| 1e         | Code: adjust non-terminal rewards                      | 5       |\n",
    "| 1f         | Code and write: adjust terminal rewards                | 5       |\n",
    "| 1g         | Written: analyze transition model                      | 5       |\n",
    "| 2a         | Written: define problem                                | 4       |\n",
    "| 2b         | Written: define states                                 | 4       |\n",
    "| 2c         | Written: define reward                                 | 4       |\n",
    "| 2d         | Written: define actions and transition                 | 4       |\n",
    "| 2e         | Written: define optimal policy                         | 4       |\n",
    "| Total      |                                                        | 60      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "# add any imports you may need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c608e4fde84b1104aa33382b2642da7",
     "grade": false,
     "grade_id": "1-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "# Problem 1: Navigating an awkward situation with grace and poise\n",
    "\n",
    "<img src='https://www.explainxkcd.com/wiki/images/5/5f/interaction.png' style=\"width: 600px;\"/>\n",
    "\n",
    "Suppose you are at a social event where you would like to avoid any interaction with a large number of the other attendees. It's not that you don't like them, it's just that you don't like *talking to* them. A few of your good friends are also in attendance, but they are tucked away in a corner. The rectangular room in which the event is being held spans gridcells at $x=1,2,\\ldots, 6$ and $y=1,2,\\ldots, 5$. At the eastern edge ($x=6$) of this first floor room, there is a balcony, with a 6-foot drop. If the event becomes unbearably awkward, you can jump off the balcony and run away. Of course, this might hurt a little bit, so we should incorporate this into our reward structure.\n",
    "\n",
    "The terminal states and rewards associated with them are given in the diagram below. The states are represented as $(x,y)$ tuples. The available actions in non-terminal states include moving exactly 1 unit North (+y), South (-y), East (+x) or West (-x), although you should not include walking into walls, because that would be embarrassing in front of all these other people. Represent actions as one of 'N', 'S', 'E', or 'W'. For now, assume all non-terminal states have a default reward of -0.01, and use a discount factor of 0.99.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "Use the following transition model for this decision process, if you are trying to move from state $s$ to state $s'$:\n",
    "* you successfully move from $s$ to $s'$ with probability 0.6\n",
    "* the remaining 0.4 probability is spread equally likely across state $s$ **and** all adjacent (N/S/E/W) states except for $s'$. Note that this does not necessarily mean that all adjacent states have 0.1, because some states do not have 4 adjacent states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "823e20cadbdae74fd72ea6227b26eb1c",
     "grade": false,
     "grade_id": "1a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1a'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1a) - 10 points\n",
    "\n",
    "Complete the `MDP` class below. The docstring comments provide some desired specifications. You may add additional methods or attributes, if you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e28782999a0a5c35e0461daeb267be5c",
     "grade": false,
     "grade_id": "mdp-class",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, nrow, ncol, terminal_states, default_reward, df):\n",
    "        '''Create/store the following attributes:\n",
    "        self.states -- a list of all the states as (x,y) tuples\n",
    "        self.terminal_states -- a dictionary with terminal state keys, and rewards as values\n",
    "        self.default_reward -- the reward for being in any non-terminal state\n",
    "        self.df -- discount factor\n",
    "        ... and anything else you decide will be useful!\n",
    "        '''\n",
    "                # YOUR CODE HERE\n",
    "        \n",
    "        self.states = []\n",
    "        self.get_states()\n",
    "        self.terminal_states = terminal_states\n",
    "        self.default_reward = default_reward\n",
    "        self.df = df\n",
    "\n",
    "#         raise NotImplementedError()\n",
    "    def get_states(self):\n",
    "        states = []\n",
    "        for i in range(1,7):\n",
    "            for j in range(1,6):\n",
    "                self.states.append((i,j))\n",
    "        return states\n",
    "    \n",
    "    def actions(self, state):\n",
    "        '''Return a list of available actions from the given state.\n",
    "        Possible actions are 'N','S','E','W'\n",
    "        [None] are the actions available from a terminal state.\n",
    "        '''\n",
    "        list_action=[]\n",
    "        x = state[0]\n",
    "        y = state[1]\n",
    "        if state in self.terminal_states:\n",
    "            list_action.append(None)\n",
    "            return list_action\n",
    "        if(x, y + 1) in self.states:\n",
    "            list_action.append('N')\n",
    "        if(x, y - 1) in self.states:\n",
    "            list_action.append('S')\n",
    "        if (x + 1, y) in self.states:\n",
    "            list_action.append('E')\n",
    "        if (x - 1, y) in self.states:\n",
    "            list_action.append('W')\n",
    "        return list_action\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    def reward(self, state):\n",
    "        '''Return the reward for being in the given state'''\n",
    "        # YOUR CODE HERE\n",
    "        if self.terminal_states.get(state):\n",
    "            return self.terminal_states.get(state)\n",
    "        else:\n",
    "            return self.default_reward\n",
    "\n",
    "    \n",
    "    def result(self, state, action):\n",
    "        '''Return the resulting state (as a tuple) from doing the given\n",
    "        action in the given state, without uncertainty. Uncertainty\n",
    "        is incorporated into the transition method.\n",
    "        state -- a tuple representing the current state\n",
    "        action -- one of N, S, E or W, as a string\n",
    "        '''\n",
    "        \n",
    "        assert action in self.actions(state), 'Error: action needs to be available in that state'\n",
    "        assert state in self.states, 'Error: invalid state'\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "#         raise NotImplementedError()\n",
    "        x = state[0]\n",
    "        y = state[1]\n",
    "        if 'N' == action:\n",
    "            return (x, y + 1)\n",
    "        elif 'S' == action:\n",
    "            return (x, y - 1)\n",
    "        elif 'E' == action:\n",
    "            return (x + 1, y)\n",
    "        elif 'W' == action:\n",
    "            return (x - 1, y)\n",
    "            \n",
    "    def transition(self, state, action):\n",
    "        '''Return a list of (probability, next_state) associated\n",
    "        with the possibilities of taking the given action from the given state.\n",
    "        '''\n",
    "        \n",
    "        if action is None:\n",
    "            # This happens for a terminal state\n",
    "            return [(0, state)]\n",
    "        else:\n",
    "            # Not a terminal state\n",
    "            length_of_actions = len(self.actions(state))\n",
    "            lst_actions = self.actions(state)\n",
    "            lst = []\n",
    "            for i in range(0, len(lst_actions)):\n",
    "#                 print(\"list_actions\",lst_actions[i])\n",
    "                if lst_actions[i] == action:\n",
    "                    probability = 0.6\n",
    "                    location =self.result(state,action)\n",
    "                    lst.append((probability,location))\n",
    "                else:\n",
    "                    probability = 0.4/length_of_actions\n",
    "                    location = self.result(state,lst_actions[i])\n",
    "                    lst.append((probability,location))\n",
    "        \n",
    "        lst.append((0.4/length_of_actions, state))\n",
    "                \n",
    "        return lst\n",
    "            \n",
    "  \n",
    "\n",
    "    def expected_utility(self, next_states, cur_util):\n",
    "        '''Return the expected utility given generated list of possible \n",
    "        next states and the current utility, which is a dictionary of the form {state : utility}\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "\n",
    "        exp_util = []\n",
    "        utility = 0\n",
    "        for key in next_states:\n",
    "            (prob, state) = key\n",
    "            utility = cur_util.get(state)\n",
    "            exp_util.append(prob*utility)\n",
    "            \n",
    "        new = sum(exp_util)\n",
    "        \n",
    "        return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75dc0b42cb21d9584de2037444b26d51",
     "grade": false,
     "grade_id": "1a-test-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1a) tests\n",
    "\n",
    "Note that these are non-exhaustive, because there is some flexibility in how the `transition` method works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e36e4589b1131c2c413d7a804bf7f4c",
     "grade": true,
     "grade_id": "1a-test-simple",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "nrow = 3\n",
    "ncol = 3\n",
    "default_reward = -0.2\n",
    "discount = 0.5\n",
    "terminal = {(1,3):-1, (1,2):2}\n",
    "mdp_simple = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "\n",
    "actions1 = set(mdp_simple.actions((2,2)))\n",
    "assert (actions1 == {'N','S','E','W'}), \"Expected set of actions is {'N','S','E','W'}, your code returned: %s\" % actions1\n",
    "\n",
    "actions2 = set(mdp_simple.actions((1,1)))\n",
    "assert (actions2 == {'N','E'}), \"Expected set of actions is {'N','E'}, your code returned: %s\" % actions2\n",
    "\n",
    "actions3 = set(mdp_simple.actions((1,2)))\n",
    "assert (actions3 == {None}), \"Expected set of actions is {None}, your code returned: %s\" % actions3\n",
    "\n",
    "reward1 = mdp_simple.reward((1,2))\n",
    "assert (reward1 == 2), \"Expected reward is 2, your code returned: %f\" % reward1\n",
    "\n",
    "reward2 = mdp_simple.reward((2,2))\n",
    "assert (reward2 == -0.2), \"Expected reward is -0.2, your code returned: %f\" % reward2\n",
    "\n",
    "result1 = mdp_simple.result((1,1), 'N')\n",
    "assert (result1 == (1,2)), \"Expected result is (1,2), your code returned: %s\" % (result1,)\n",
    "\n",
    "print(\"All tests passed: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe6b6124959228ec56ece931028694a1",
     "grade": true,
     "grade_id": "1a-test-problem",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "# set values from problem statement\n",
    "nrow = 5\n",
    "ncol = 6\n",
    "default_reward = -0.01\n",
    "discount = 0.99\n",
    "terminal = {(1,3):-1, (1,4):2, (1,5):2, (2,1):-1, (3,1):-1, (3,4):-1, (3,5):1,\n",
    "            (4,3):-1, (4,4):-1, (6,1):-5, (6,2):-5, (6,3):-5, (6,4):-5, (6,5):-5}\n",
    "mdp_p1 = MDP(nrow, ncol, terminal, default_reward, discount)\n",
    "\n",
    "# Find the expected utility of walking N from (1,1):\n",
    "util_old = {s : s[0]+s[1] for s in mdp_p1.states}\n",
    "\n",
    "next_states = mdp_p1.transition((1,1), 'N')\n",
    "assert (len(next_states) == 3), \"Expected 3 possible next states, your code returned: %d\" % len(next_states)\n",
    "\n",
    "exp_util = mdp_p1.expected_utility(next_states, util_old)\n",
    "assert (exp_util == 2.8), \"Expected utility should be 2.8, your code returned %f\" % exp_util\n",
    "\n",
    "print(\"All tests passed: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1171844cdb0baf1260559a6f58a18477",
     "grade": false,
     "grade_id": "1b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1b'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1b) - 5 points\n",
    "\n",
    "Implement value iteration to calculate the utilities for each state.  Also implement a function that takes as arguments an `MDP` object and a dictionary of state-utility pairs (key-value) and returns a dictionary for the optimal policy.  The optimal policy dictionary should have state tuples as keys and the optimal move (None, N, S, E or W) as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85c8c83a0cd970ca44ba24d8f8715781",
     "grade": false,
     "grade_id": "value-iter-find-policy",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(mdp, tol=1e-3):\n",
    "    # TODO: \n",
    "    # 1. initialize utility for all states to 0\n",
    "    # 2. for each state on the board\n",
    "    #    2.1. calculate expected utility for each possible next state\n",
    "    #    2.2. find best utility out of possible expected utilities\n",
    "    #    2.3. define new utility of the state\n",
    "    # 3. Repeat 2 until problem is converged\n",
    "    \n",
    "    empty_dict = {} #initialize utility for all states to 0 for each state on the board\n",
    "    states = mdp.states\n",
    "    terminal_dic = mdp.terminal_states\n",
    "    i = 0\n",
    "    while i < len(states):\n",
    "        empty_dict.update({states[i]:0})\n",
    "        i += 1\n",
    "    for index, key in enumerate(empty_dict):\n",
    "        if key in list(terminal_dic.keys()):\n",
    "            empty_dict[key] = terminal_dic[key]  \n",
    "    \n",
    "    while True:\n",
    "        lst_change = []\n",
    "        for key in states:\n",
    "            old_util = empty_dict.get(key)\n",
    "            change = 0\n",
    "            if key in mdp.terminal_states:\n",
    "                continue\n",
    "            else:\n",
    "                lst_actions = mdp.actions(key)\n",
    "                lst_utl = []\n",
    "                max_utl = 0\n",
    "                for action in lst_actions:\n",
    "                    lst_probability_next_states = mdp.transition(key,action)\n",
    "\n",
    "                    utl_value = mdp.expected_utility(lst_probability_next_states, empty_dict)\n",
    "\n",
    "                    lst_utl.append(mdp.reward(key) + mdp.df*utl_value)     \n",
    "            max_utl = max(lst_utl)\n",
    "            empty_dict[key] = max_utl\n",
    "\n",
    "            change = old_util - max_utl\n",
    "            lst_change.append(change)\n",
    "            \n",
    "        if max(lst_change) == 0:\n",
    "            break\n",
    "\n",
    "    return empty_dict\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "def find_policy(mdp, utility):\n",
    "    '''Return a dictionary containing the best policy for each state s,\n",
    "    of the form {s : s_policy}\n",
    "    '''\n",
    "    best_policy_dict = {}\n",
    "    \n",
    "    for key in utility:\n",
    "        x = key[0]\n",
    "        y = key[1]\n",
    "        lst_actions = mdp.actions(key)\n",
    "        policy_dict = {}\n",
    "        for action in lst_actions:\n",
    "            if action == 'N':\n",
    "                policy_dict[action] = utility.get((x, y+1))\n",
    "            elif action == 'S':\n",
    "                policy_dict[action] = utility.get((x, y-1))\n",
    "            elif action == 'E':\n",
    "                policy_dict[action] = utility.get((x+1, y))\n",
    "            elif action == 'W':\n",
    "                policy_dict[action] = utility.get((x-1, y))\n",
    "                \n",
    "        current_max = -math.inf\n",
    "        best_action = \"\"\n",
    "        for i in policy_dict:\n",
    "            if policy_dict.get(i) > current_max:\n",
    "                current_max = policy_dict.get(i)\n",
    "                best_action = i\n",
    "                \n",
    "        best_policy_dict[key] = best_action\n",
    "#     print(best_policy_dict)        \n",
    "    return best_policy_dict\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f14e15e8c0ad815923f021a8797e81e",
     "grade": false,
     "grade_id": "1b-tests-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1b) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54d20c4e3b584c7e7863d35660db1f6c",
     "grade": true,
     "grade_id": "1b-tests-asserts",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed test cases: 5 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "utility = value_iteration(mdp_p1, tol=1e-6)\n",
    "policy = find_policy(mdp_p1, utility)\n",
    "\n",
    "util1 = utility[(1,5)]\n",
    "assert (util1 == 2), \"Expected utility of 2, your code returned %f\" % util1\n",
    "\n",
    "util2 = utility[(6,1)]\n",
    "assert (util2 == -5), \"Expected utility of -5, your code returned %f\" % util2\n",
    "\n",
    "util3 = round(utility[(2,5)],2)\n",
    "assert (util3 == 1.74), \"Expected utility of 1.74, your code returned %f\" % util3\n",
    "\n",
    "util4 = round(utility[(5,3)],2)\n",
    "assert (util4 == -1.39), \"Expected utility of -1.39, your code returned %f\" % util4\n",
    "\n",
    "policy1 = policy[(2,4)]\n",
    "assert (policy1 == 'W'), \"Expected policy is 'W', your code returned %s\" % policy1\n",
    "\n",
    "policy2 = policy[(1,1)]\n",
    "assert (policy2 == 'N'), \"Expected policy is 'N', your code returned %s\" % policy2\n",
    "\n",
    "print(\"Passed test cases: 5 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1b6b83bb256c438e69fc7bb280e5a80",
     "grade": false,
     "grade_id": "1c-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1c'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1c) - 5 points\n",
    "\n",
    "If we enter the room at $s_0$, what is the optimal path for us to follow? Complete the following function to generate the sequence of states along the path. If we start in state $s_0$, then your output should be in the form $[s_0, s_1, s_2, ... , s_{term}]$ where $s_{term}$ is a terminal state. Set your tolerance for value iteration to be $10^{-6}$\n",
    "\n",
    "Additionally, create a graphic to illustrate this policy pathway, either by generating a plot in Python or by uploading a hand-drawn image and including a link to it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "740f51d6586bfb8d597b3afe8e46b108",
     "grade": false,
     "grade_id": "1c-answer-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_optimal_path(mdp, state):\n",
    "    '''Generate list of states visited along the optimal path given an MDP\n",
    "    instance and the starting state\n",
    "    '''\n",
    "    util = value_iteration(mdp, tol=1e-6)\n",
    "    policy = find_policy(mdp,utility)\n",
    "    lst_path = []\n",
    "    lst_path.append(state)\n",
    "    while True:\n",
    "        action = policy[state]\n",
    "        if action == '':\n",
    "            break\n",
    "        else:\n",
    "            state = mdp.result(state,action)\n",
    "            lst_path.append(state)\n",
    "    print(lst_path)\n",
    "    return lst_path\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "085e7a8e272b36a76e8ce82e1537152e",
     "grade": false,
     "grade_id": "1c-answer-graphic",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Put a link to your graphic below for the path from (5,3). You can find an [example image here](http://www.cs.colorado.edu/~tonyewong/home/resources/hw06_mdp_path.png) with the optimal path starting from (5,1). **Please include a link rather than attaching a file**. This can be a link to your file in Google Drive, with the permissions set to public. The graders will not ask for permissions! The syntax for including a link in markdown is `[link text](url.com/to/your/image)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bd08159d2a99ee23492b15403f5e025",
     "grade": true,
     "grade_id": "1c-graphic",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE: https://drive.google.com/file/d/1A4MfKrPNZIhIHXLLWC008v8CI_pAISim/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f5c9f5250a8cb8db949d1166219cf06",
     "grade": true,
     "grade_id": "1c-tests-asserts",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]\n",
      "[(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]\n",
      "[(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)]\n",
      "Passed tests: 3 points\n"
     ]
    }
   ],
   "source": [
    "## BEGIN TESTS\n",
    "path1 = find_optimal_path(mdp_p1, (5,3))\n",
    "assert (path1 == [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 3), (5, 2), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path1\n",
    "\n",
    "path2 = find_optimal_path(mdp_p1, (5,1))\n",
    "assert (path2 == [(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(5, 1), (4, 1), (4, 2), (3, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path2\n",
    "\n",
    "path3 = find_optimal_path(mdp_p1, (1,1))\n",
    "assert (path3 == [(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)]), \"The optimal path is [(1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (1, 4)], your code generated %s\" % path3\n",
    "\n",
    "print(\"Passed tests: 3 points\")\n",
    "## END TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b468abfa8603618ab21ea57d3a26d564",
     "grade": false,
     "grade_id": "1d-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1d'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1d) - 5 points\n",
    "\n",
    "From (3,2) the optimal move is to walk West. If we are trying to go talk to our friends in the Northwest corner, why would we rather do this than walk North first, then West?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75467f69d87838440cb5b2a3dd342a34",
     "grade": true,
     "grade_id": "1d-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE: If you go north the agent have a higher probability of landing on a negative terminal state like (3,4) or (4,3). Thats way its a better choice to go west. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "745fc8b0c5373d58b894072f14d2bcf8",
     "grade": false,
     "grade_id": "1e-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1e'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1e) - 5 points\n",
    "\n",
    "How painfully awkward do you need to set the default reward for non-terminal states before the optimal move at (5,1) becomes jumping off the balcony immediately and running away? Implement the following function which returns the reward where the policy for (5,1) is to jump off the balcony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83a84df7c9004254b67cada546239f68",
     "grade": false,
     "grade_id": "1e-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_non_terminal_reward():\n",
    "    for reward in np.arange(-0.01, -3, -0.01):\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4346aaa5a01e06b338b3482b225aaa30",
     "grade": true,
     "grade_id": "1e-tests-assert",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7f848694a119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreward1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_non_terminal_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2.09\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The expected reward is -2.09, your code returned %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreward1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test passed: 5 points\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2ca3a22eace9>\u001b[0m in \u001b[0;36mfind_non_terminal_reward\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward1 = find_non_terminal_reward()\n",
    "assert (reward1 == -2.09), \"The expected reward is -2.09, your code returned %f\" % reward1\n",
    "\n",
    "print(\"Test passed: 5 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e194e2f330afbd4fe1437648f6f86723",
     "grade": false,
     "grade_id": "1f-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1f'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1f) - 5 points\n",
    "\n",
    "In **1e** we assumed a certain level of loss (negative reward) just for being present.  But a more realistic approach might be to instead change the reward structure for the terminal states. Consider the terminal states with -1 reward in the default model. Let $R^*$ denote the reward associated with these states. How low does $R^*$ need to be in order for us to immediately jump off the balcony and run away? Use the default non-terminal state reward of -0.01. Implement the following function to return the value of $R^*$ which leads to a policy of jumping off the balcony at (5,1). Write a few sentences interpreting your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03cd66008c7b709512677ede48059600",
     "grade": false,
     "grade_id": "1f-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_terminal_reward():\n",
    "    \n",
    "    for rstar in np.arange(-6, -12, -0.01):\n",
    "        # TODO:\n",
    "        # 1. set the reward of the terminal nodes \n",
    "        # 2. define MDP with appropriate parameters\n",
    "        # 3. find policy for state (5,1)\n",
    "        # 4. return R* if the policy for (5,1) is to jump off the balcony\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7008fb02d7df7ba16c7882585e9b890d",
     "grade": false,
     "grade_id": "1f-test-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1f) tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5230fc87068ca9643b6d7333441b715",
     "grade": true,
     "grade_id": "1f-test-asserts",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "reward1 = round(find_terminal_reward(),2)\n",
    "assert (reward1 == -11.39), \"Expected reward is -11.39, your code returned %f\" % reward1\n",
    "\n",
    "print(\"Passed test: 3 points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b7fd68f561b33ad94ed8a286a7eb3d6",
     "grade": false,
     "grade_id": "1f-reflection-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a few sentences with your interpretation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12e68d9c45421d6fd097011fd0b9f8bd",
     "grade": true,
     "grade_id": "1f-answer",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d2e45eceb59f1d36e0c528cd7faf22b",
     "grade": false,
     "grade_id": "1g-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p1g'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "## (1g) - 5 points\n",
    "\n",
    "Given the problem context, write a few sentences about why this is or is not an appropriate transition model. Include an interpretation of the terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a884715306f6da7b8d2b7fd566f7732e",
     "grade": true,
     "grade_id": "1g-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5038f4053461860ec0728cc4626f059b",
     "grade": false,
     "grade_id": "2-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "<a id='p2'></a>\n",
    "[Back to top](#top)\n",
    "\n",
    "# Problem 2: Define your very own MDP\n",
    "\n",
    "For this problem, you do not need to write any code, but rather communicate your ideas clearly using complete sentences and descriptions of the concepts the questions ask about. You can, of course, include some pseudocode if it helps, but that is not strictly necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12e0af4a88a181762bd42ae609e93cf",
     "grade": false,
     "grade_id": "2a-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2a'></a>\n",
    "\n",
    "## (2a) - 4 points\n",
    "\n",
    "Describe something you think would be interesting to model using a Markov decision process.  Be **creative** - do not use any examples from your homework, class, or the textbook, and if you are working with other students, please **come up with your own example**. There are so, SO many possible answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d13a848d94d3a2552c856f613b1fd2d",
     "grade": true,
     "grade_id": "2a-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Honey Bee World - is a 3 x 4 grid with a bee as an agent. The bee's goal is to reach the hive in the top right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33940776445caeaffd19100073cef2d0",
     "grade": false,
     "grade_id": "2b-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2b'></a>\n",
    "\n",
    "## (2b) - 4 points\n",
    "\n",
    "What are the states associated with your MDP? Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c536e75b992639fa2b21877a388eda0f",
     "grade": true,
     "grade_id": "2b-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The grid world is filled with grass states, fire states and a hive goal state. Terminal states are the hive and the fire. Non Terminal states are the flower and grass states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05b9b36293b64ab2d3ba757666ac3427",
     "grade": false,
     "grade_id": "2c-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2c'></a>\n",
    "\n",
    "## (2c) - 4 points\n",
    "\n",
    "What is the reward structure associated with your MDP?  Include a discussion of terminal/non-terminal states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "90b36dc1c0b6d151da8f42c1d93b9d9a",
     "grade": true,
     "grade_id": "2c-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "### Reward Model\n",
    "- moveReward = -5 // For every action the bee moves\n",
    "- hiveReward = +100// terminal state\n",
    "- fire = -50 // terminal state\n",
    "- flower = +5  // extra reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "565e36ab6960052d86fa2a08c18f96c4",
     "grade": false,
     "grade_id": "2d-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2d'></a>\n",
    "\n",
    "## (2d) - 4 points\n",
    "\n",
    "What are the actions and transition model associated with your MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b16a36bb225d610a1837822e543162f1",
     "grade": true,
     "grade_id": "2d-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Actions\n",
    "- up\n",
    "- down\n",
    "- left\n",
    "- right\n",
    "\n",
    "Transition model:\n",
    "\n",
    "The Bee start out in state (1,1). It can move to \"up\" with a probability of 0.4 or move to the right with a prob 0.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35ab2fc9b4eb20ce1b32e1a2aa0e1007",
     "grade": false,
     "grade_id": "2e-text",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='p2e'></a>\n",
    "\n",
    "## (2e) - 4 points\n",
    "\n",
    "Interpret what an optimal policy represents in the context of your particular MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c57bad2508d2aa53541aaaa24b5f55c3",
     "grade": true,
     "grade_id": "2e-answer",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The bee will choice the optimal path by collecting as many flowers as possible before reaching the hive goal state and with out landing on fire terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "519px",
    "left": "22px",
    "top": "149px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
